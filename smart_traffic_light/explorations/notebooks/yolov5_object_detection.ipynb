{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umuepe6-w2rS",
        "outputId": "930fd625-ac6f-4af6-cd42-0dcbe559974a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Checking if gpu is available or not\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTvDNSILZoN9",
        "outputId": "146ea462-9f45-4d2e-d203-c75ed6fb16f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 11996, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 11996 (delta 4), reused 7 (delta 4), pack-reused 11987\u001b[K\n",
            "Receiving objects: 100% (11996/11996), 65.29 MiB | 13.75 MiB/s, done.\n",
            "Resolving deltas: 100% (7951/7951), done.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "/content/yolov5\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 KB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Setup complete. Using torch 1.13.1+cu116 (Tesla T4)\n"
          ]
        }
      ],
      "source": [
        "#clone YOLOv5 and \n",
        "!git clone https://github.com/prabhupad26/yolov5.git  # clone repo\n",
        "!git checkout feature/training_changes\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "%pip install -q roboflow\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2jjT5uIHo6l5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "423aa2e7-b8a8-44ce-c617-1b61ab38a601"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in /content/datasets/smart-traffic-light-27 to yolov5pytorch: 100% [152366394 / 152366394] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to /content/datasets/smart-traffic-light-27 in yolov5pytorch:: 100%|██████████| 2006/2006 [00:01<00:00, 1616.24it/s]\n"
          ]
        }
      ],
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(model_format=\"yolov5\", notebook=\"ultralytics\", api_key=\"HDll2oFm5X8KK35ziZJL\")\n",
        "\n",
        "# set up environment\n",
        "os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\"\n",
        "\n",
        "#after following the link above, recieve python code with these fields filled in\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"HDll2oFm5X8KK35ziZJL\")\n",
        "project = rf.workspace().project(\"casestudy1/smart-traffic-light\")\n",
        "dataset = project.version(\"27\").download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7yAi9hd-T4B"
      },
      "source": [
        "CLI arguments : \n",
        "- **img:** define input image size\n",
        "- **batch:** determine batch size\n",
        "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
        "- **data:** Our dataset locaiton is saved in the `dataset.location`\n",
        "- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n",
        "- **cache:** cache images for faster training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcYDbWgOKtYg",
        "outputId": "9f3d55a3-21d9-460d-8036-5b42a0ac050b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets/smart-traffic-light-27\n"
          ]
        }
      ],
      "source": [
        "!echo {dataset.location}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eaFNnxLJbq4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eeefdb5-3cae-4e78-f589-bd470c7dfd86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/datasets/smart-traffic-light-27/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=400, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[10], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "remote: Enumerating objects: 3367, done.\u001b[K\n",
            "remote: Counting objects: 100% (1901/1901), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 3367 (delta 1895), reused 1894 (delta 1892), pack-reused 1466\u001b[K\n",
            "Receiving objects: 100% (3367/3367), 473.89 KiB | 2.12 MiB/s, done.\n",
            "Resolving deltas: 100% (2574/2574), completed with 190 local objects.\n",
            "From https://github.com/ultralytics/yolov5\n",
            " * [new branch]      add/weights_dir   -> ultralytics/add/weights_dir\n",
            " * [new branch]      benchmarks        -> ultralytics/benchmarks\n",
            " * [new branch]      exp/scaleFill     -> ultralytics/exp/scaleFill\n",
            " * [new branch]      exp12             -> ultralytics/exp12\n",
            " * [new branch]      exp13             -> ultralytics/exp13\n",
            " * [new branch]      exp13-nosoft      -> ultralytics/exp13-nosoft\n",
            " * [new branch]      exp13-soft        -> ultralytics/exp13-soft\n",
            " * [new branch]      fix/rgb_albumentations -> ultralytics/fix/rgb_albumentations\n",
            " * [new branch]      ghost             -> ultralytics/ghost\n",
            " * [new branch]      master            -> ultralytics/master\n",
            " * [new branch]      study_activations -> ultralytics/study_activations\n",
            " * [new branch]      ultralytics/HUB   -> ultralytics/ultralytics/HUB\n",
            " * [new branch]      update/cls-album  -> ultralytics/update/cls-album\n",
            " * [new branch]      update/textlogger -> ultralytics/update/textlogger\n",
            " * [new branch]      update/threaded   -> ultralytics/update/threaded\n",
            " * [new tag]         v1.0              -> v1.0\n",
            " * [new tag]         v2.0              -> v2.0\n",
            " * [new tag]         v3.0              -> v3.0\n",
            " * [new tag]         v3.1              -> v3.1\n",
            " * [new tag]         v4.0              -> v4.0\n",
            " * [new tag]         v5.0              -> v5.0\n",
            " * [new tag]         v6.0              -> v6.0\n",
            " * [new tag]         v6.1              -> v6.1\n",
            " * [new tag]         v6.2              -> v6.2\n",
            " * [new tag]         v7.0              -> v7.0\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0m⚠️ YOLOv5 is out of date by 5 commits. Use 'git pull ultralytics master' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
            "YOLOv5 🚀 v7.0-117-g85f6019 Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "2023-03-23 12:59:00.796998: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-23 12:59:01.700654: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-23 12:59:01.700777: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-23 12:59:01.700804: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 24.7MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 301MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=7\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     32364  models.yolo.Detect                      [7, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7038508 parameters, 7038508 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "freezing model.0.conv.weight\n",
            "freezing model.0.bn.weight\n",
            "freezing model.0.bn.bias\n",
            "freezing model.1.conv.weight\n",
            "freezing model.1.bn.weight\n",
            "freezing model.1.bn.bias\n",
            "freezing model.2.cv1.conv.weight\n",
            "freezing model.2.cv1.bn.weight\n",
            "freezing model.2.cv1.bn.bias\n",
            "freezing model.2.cv2.conv.weight\n",
            "freezing model.2.cv2.bn.weight\n",
            "freezing model.2.cv2.bn.bias\n",
            "freezing model.2.cv3.conv.weight\n",
            "freezing model.2.cv3.bn.weight\n",
            "freezing model.2.cv3.bn.bias\n",
            "freezing model.2.m.0.cv1.conv.weight\n",
            "freezing model.2.m.0.cv1.bn.weight\n",
            "freezing model.2.m.0.cv1.bn.bias\n",
            "freezing model.2.m.0.cv2.conv.weight\n",
            "freezing model.2.m.0.cv2.bn.weight\n",
            "freezing model.2.m.0.cv2.bn.bias\n",
            "freezing model.3.conv.weight\n",
            "freezing model.3.bn.weight\n",
            "freezing model.3.bn.bias\n",
            "freezing model.4.cv1.conv.weight\n",
            "freezing model.4.cv1.bn.weight\n",
            "freezing model.4.cv1.bn.bias\n",
            "freezing model.4.cv2.conv.weight\n",
            "freezing model.4.cv2.bn.weight\n",
            "freezing model.4.cv2.bn.bias\n",
            "freezing model.4.cv3.conv.weight\n",
            "freezing model.4.cv3.bn.weight\n",
            "freezing model.4.cv3.bn.bias\n",
            "freezing model.4.m.0.cv1.conv.weight\n",
            "freezing model.4.m.0.cv1.bn.weight\n",
            "freezing model.4.m.0.cv1.bn.bias\n",
            "freezing model.4.m.0.cv2.conv.weight\n",
            "freezing model.4.m.0.cv2.bn.weight\n",
            "freezing model.4.m.0.cv2.bn.bias\n",
            "freezing model.4.m.1.cv1.conv.weight\n",
            "freezing model.4.m.1.cv1.bn.weight\n",
            "freezing model.4.m.1.cv1.bn.bias\n",
            "freezing model.4.m.1.cv2.conv.weight\n",
            "freezing model.4.m.1.cv2.bn.weight\n",
            "freezing model.4.m.1.cv2.bn.bias\n",
            "freezing model.5.conv.weight\n",
            "freezing model.5.bn.weight\n",
            "freezing model.5.bn.bias\n",
            "freezing model.6.cv1.conv.weight\n",
            "freezing model.6.cv1.bn.weight\n",
            "freezing model.6.cv1.bn.bias\n",
            "freezing model.6.cv2.conv.weight\n",
            "freezing model.6.cv2.bn.weight\n",
            "freezing model.6.cv2.bn.bias\n",
            "freezing model.6.cv3.conv.weight\n",
            "freezing model.6.cv3.bn.weight\n",
            "freezing model.6.cv3.bn.bias\n",
            "freezing model.6.m.0.cv1.conv.weight\n",
            "freezing model.6.m.0.cv1.bn.weight\n",
            "freezing model.6.m.0.cv1.bn.bias\n",
            "freezing model.6.m.0.cv2.conv.weight\n",
            "freezing model.6.m.0.cv2.bn.weight\n",
            "freezing model.6.m.0.cv2.bn.bias\n",
            "freezing model.6.m.1.cv1.conv.weight\n",
            "freezing model.6.m.1.cv1.bn.weight\n",
            "freezing model.6.m.1.cv1.bn.bias\n",
            "freezing model.6.m.1.cv2.conv.weight\n",
            "freezing model.6.m.1.cv2.bn.weight\n",
            "freezing model.6.m.1.cv2.bn.bias\n",
            "freezing model.6.m.2.cv1.conv.weight\n",
            "freezing model.6.m.2.cv1.bn.weight\n",
            "freezing model.6.m.2.cv1.bn.bias\n",
            "freezing model.6.m.2.cv2.conv.weight\n",
            "freezing model.6.m.2.cv2.bn.weight\n",
            "freezing model.6.m.2.cv2.bn.bias\n",
            "freezing model.7.conv.weight\n",
            "freezing model.7.bn.weight\n",
            "freezing model.7.bn.bias\n",
            "freezing model.8.cv1.conv.weight\n",
            "freezing model.8.cv1.bn.weight\n",
            "freezing model.8.cv1.bn.bias\n",
            "freezing model.8.cv2.conv.weight\n",
            "freezing model.8.cv2.bn.weight\n",
            "freezing model.8.cv2.bn.bias\n",
            "freezing model.8.cv3.conv.weight\n",
            "freezing model.8.cv3.bn.weight\n",
            "freezing model.8.cv3.bn.bias\n",
            "freezing model.8.m.0.cv1.conv.weight\n",
            "freezing model.8.m.0.cv1.bn.weight\n",
            "freezing model.8.m.0.cv1.bn.bias\n",
            "freezing model.8.m.0.cv2.conv.weight\n",
            "freezing model.8.m.0.cv2.bn.weight\n",
            "freezing model.8.m.0.cv2.bn.bias\n",
            "freezing model.9.cv1.conv.weight\n",
            "freezing model.9.cv1.bn.weight\n",
            "freezing model.9.cv1.bn.bias\n",
            "freezing model.9.cv2.conv.weight\n",
            "freezing model.9.cv2.bn.weight\n",
            "freezing model.9.cv2.bn.bias\n",
            "WARNING ⚠️ --img-size 400 must be multiple of max stride 32, updating to 416\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/smart-traffic-light-27/train/labels... 797 images, 0 backgrounds, 0 corrupt: 100% 797/797 [00:00<00:00, 1939.03it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/smart-traffic-light-27/train/labels.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.3GB ram): 100% 797/797 [00:11<00:00, 68.77it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/smart-traffic-light-27/valid/labels... 105 images, 0 backgrounds, 0 corrupt: 100% 105/105 [00:00<00:00, 492.55it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/smart-traffic-light-27/valid/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100% 105/105 [00:03<00:00, 32.63it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.03 anchors/target, 0.998 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/49     0.484G     0.1221    0.03242    0.05987        103        416:   0% 0/50 [00:01<?, ?it/s]WARNING ⚠️ TensorBoard graph visualization failure Sizes of tensors must match except in dimension 1. Expected size 26 but got size 25 for tensor number 1 in the list.\n",
            "       0/49     0.623G     0.1031    0.04342    0.05433         81        416: 100% 50/50 [00:12<00:00,  3.96it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:02<00:00,  1.94it/s]\n",
            "                   all        105        391       0.35      0.217      0.054     0.0208\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/49      1.17G    0.07643    0.04273    0.04341         62        416: 100% 50/50 [00:08<00:00,  5.69it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  4.45it/s]\n",
            "                   all        105        391      0.621      0.243      0.191      0.087\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/49      1.17G    0.06934    0.03974    0.03747        126        416: 100% 50/50 [00:08<00:00,  6.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.85it/s]\n",
            "                   all        105        391      0.354      0.333      0.225     0.0936\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/49      1.17G    0.06382    0.03809     0.0351        112        416: 100% 50/50 [00:07<00:00,  6.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.33it/s]\n",
            "                   all        105        391      0.252      0.429       0.31      0.158\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/49      1.17G    0.05889    0.03795    0.03044         62        416: 100% 50/50 [00:08<00:00,  5.85it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.06it/s]\n",
            "                   all        105        391      0.406      0.542      0.465      0.218\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/49      1.17G    0.05489     0.0372    0.02757        162        416: 100% 50/50 [00:08<00:00,  5.75it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.49it/s]\n",
            "                   all        105        391      0.557      0.556      0.561      0.298\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/49      1.17G    0.05158    0.03568     0.0249         93        416: 100% 50/50 [00:07<00:00,  6.91it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.84it/s]\n",
            "                   all        105        391      0.478      0.613      0.551      0.269\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/49      1.17G    0.05037    0.03596    0.02218        126        416: 100% 50/50 [00:08<00:00,  5.88it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  4.91it/s]\n",
            "                   all        105        391      0.616      0.623      0.639      0.357\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/49      1.17G    0.04795    0.03662    0.02045         86        416: 100% 50/50 [00:08<00:00,  5.64it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.85it/s]\n",
            "                   all        105        391      0.645      0.642      0.669      0.367\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/49      1.17G    0.04698    0.03452    0.01829        116        416: 100% 50/50 [00:07<00:00,  6.92it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  4.39it/s]\n",
            "                   all        105        391      0.648      0.655      0.624      0.353\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/49      1.17G    0.04538    0.03531    0.01709         76        416: 100% 50/50 [00:07<00:00,  6.28it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.34it/s]\n",
            "                   all        105        391      0.804      0.643      0.759      0.421\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      11/49      1.17G     0.0463    0.03604    0.01491        137        416: 100% 50/50 [00:08<00:00,  5.71it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.90it/s]\n",
            "                   all        105        391      0.772      0.649      0.731      0.413\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      12/49      1.17G     0.0455    0.03629    0.01453         69        416: 100% 50/50 [00:07<00:00,  6.49it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.53it/s]\n",
            "                   all        105        391      0.772      0.648      0.721      0.427\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      13/49      1.17G    0.04402    0.03473     0.0133         77        416: 100% 50/50 [00:07<00:00,  6.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.74it/s]\n",
            "                   all        105        391      0.799      0.691      0.763      0.423\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      14/49      1.17G    0.04323    0.03435    0.01317         77        416: 100% 50/50 [00:08<00:00,  5.79it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.45it/s]\n",
            "                   all        105        391      0.745      0.727      0.747      0.419\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      15/49      1.17G    0.04329    0.03437     0.0125        114        416: 100% 50/50 [00:08<00:00,  5.81it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  4.33it/s]\n",
            "                   all        105        391      0.827      0.728      0.791      0.441\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      16/49      1.17G    0.04206    0.03424    0.01185        110        416: 100% 50/50 [00:07<00:00,  6.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  6.18it/s]\n",
            "                   all        105        391      0.785      0.706      0.767      0.448\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      17/49      1.17G     0.0419    0.03505    0.01127        134        416: 100% 50/50 [00:08<00:00,  5.67it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  6.03it/s]\n",
            "                   all        105        391      0.808      0.756      0.794      0.453\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      18/49      1.17G    0.04168    0.03503    0.01109         54        416: 100% 50/50 [00:08<00:00,  5.77it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  4.57it/s]\n",
            "                   all        105        391      0.763      0.756      0.772      0.476\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      19/49      1.17G     0.0411    0.03546    0.01038         57        416: 100% 50/50 [00:07<00:00,  6.80it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.73it/s]\n",
            "                   all        105        391      0.817      0.742      0.798      0.455\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      20/49      1.17G     0.0399    0.03401    0.01033         97        416: 100% 50/50 [00:08<00:00,  5.88it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.94it/s]\n",
            "                   all        105        391      0.796      0.731      0.783      0.481\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      21/49      1.17G    0.03992    0.03579    0.01006        175        416: 100% 50/50 [00:08<00:00,  5.70it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.70it/s]\n",
            "                   all        105        391      0.827      0.723      0.797      0.481\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      22/49      1.17G    0.03982    0.03415   0.009484         56        416: 100% 50/50 [00:07<00:00,  6.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.72it/s]\n",
            "                   all        105        391      0.806       0.79      0.818      0.488\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      23/49      1.17G    0.03904    0.03271   0.009395         89        416: 100% 50/50 [00:07<00:00,  6.70it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.97it/s]\n",
            "                   all        105        391      0.795      0.756      0.805       0.48\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      24/49      1.17G    0.03893    0.03331   0.009324         62        416: 100% 50/50 [00:08<00:00,  5.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  6.17it/s]\n",
            "                   all        105        391      0.764      0.765      0.783      0.472\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      25/49      1.17G      0.039    0.03373   0.008965        109        416: 100% 50/50 [00:08<00:00,  6.04it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.78it/s]\n",
            "                   all        105        391      0.838      0.764      0.802      0.488\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      26/49      1.17G    0.03811    0.03384   0.008509        142        416: 100% 50/50 [00:07<00:00,  6.72it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.89it/s]\n",
            "                   all        105        391      0.786      0.764      0.798      0.485\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      27/49      1.17G     0.0385    0.03283   0.008608         96        416: 100% 50/50 [00:08<00:00,  5.68it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.31it/s]\n",
            "                   all        105        391      0.864      0.714      0.792      0.502\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      28/49      1.17G    0.03705    0.03216   0.008396         77        416: 100% 50/50 [00:08<00:00,  5.83it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.30it/s]\n",
            "                   all        105        391      0.867      0.757      0.837      0.518\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      29/49      1.17G    0.03752    0.03201   0.009017        177        416: 100% 50/50 [00:07<00:00,  6.94it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.72it/s]\n",
            "                   all        105        391      0.812      0.769      0.814      0.496\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      30/49      1.17G    0.03729    0.03283   0.007941        118        416: 100% 50/50 [00:08<00:00,  5.81it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.59it/s]\n",
            "                   all        105        391      0.814      0.752      0.798       0.49\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      31/49      1.17G     0.0367     0.0324   0.007815        102        416: 100% 50/50 [00:08<00:00,  5.73it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.65it/s]\n",
            "                   all        105        391      0.807      0.795      0.822      0.504\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      32/49      1.17G     0.0367    0.03356   0.007498         70        416: 100% 50/50 [00:07<00:00,  6.85it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.84it/s]\n",
            "                   all        105        391       0.81      0.784      0.821        0.5\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      33/49      1.17G     0.0362    0.03133   0.007817         93        416: 100% 50/50 [00:08<00:00,  5.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.92it/s]\n",
            "                   all        105        391      0.829      0.758      0.811      0.501\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      34/49      1.17G    0.03586     0.0324   0.007146         78        416: 100% 50/50 [00:08<00:00,  5.76it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.86it/s]\n",
            "                   all        105        391      0.831      0.776      0.815      0.502\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      35/49      1.17G    0.03573    0.03195   0.007112        151        416: 100% 50/50 [00:07<00:00,  6.77it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  4.19it/s]\n",
            "                   all        105        391      0.804      0.762      0.814      0.502\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      36/49      1.17G    0.03543    0.03104   0.007076         73        416: 100% 50/50 [00:07<00:00,  6.30it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.81it/s]\n",
            "                   all        105        391      0.833      0.769      0.821      0.502\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      37/49      1.17G    0.03524     0.0325   0.006944        119        416: 100% 50/50 [00:08<00:00,  5.63it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.61it/s]\n",
            "                   all        105        391      0.827      0.752      0.818      0.506\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      38/49      1.17G    0.03543    0.03223   0.007302        149        416: 100% 50/50 [00:07<00:00,  6.43it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.74it/s]\n",
            "                   all        105        391      0.845      0.767      0.823      0.507\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      39/49      1.17G    0.03427    0.03052   0.006244        106        416: 100% 50/50 [00:07<00:00,  6.73it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.34it/s]\n",
            "                   all        105        391      0.858       0.77      0.826      0.518\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      40/49      1.17G    0.03441    0.03052   0.006944         85        416: 100% 50/50 [00:08<00:00,  5.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.65it/s]\n",
            "                   all        105        391      0.838      0.778      0.821      0.516\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      41/49      1.17G    0.03484    0.03087   0.006258         51        416: 100% 50/50 [00:08<00:00,  6.21it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.89it/s]\n",
            "                   all        105        391      0.843      0.787      0.835      0.527\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      42/49      1.17G    0.03461     0.0314   0.006087        111        416: 100% 50/50 [00:07<00:00,  6.69it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  6.00it/s]\n",
            "                   all        105        391      0.831      0.787      0.827      0.518\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      43/49      1.17G    0.03415    0.03076   0.005826         71        416: 100% 50/50 [00:08<00:00,  5.65it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  6.05it/s]\n",
            "                   all        105        391      0.791      0.798      0.821      0.517\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      44/49      1.17G    0.03445    0.03079    0.00552         78        416: 100% 50/50 [00:08<00:00,  5.76it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.62it/s]\n",
            "                   all        105        391      0.848       0.79      0.832      0.521\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      45/49      1.17G    0.03317    0.02908   0.005709         94        416: 100% 50/50 [00:07<00:00,  6.95it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.39it/s]\n",
            "                   all        105        391      0.834      0.786      0.831      0.526\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      46/49      1.17G    0.03404    0.03161   0.005788        128        416: 100% 50/50 [00:08<00:00,  5.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.93it/s]\n",
            "                   all        105        391      0.848      0.778      0.831      0.516\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      47/49      1.17G    0.03364    0.02898   0.006057         79        416: 100% 50/50 [00:08<00:00,  5.65it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.10it/s]\n",
            "                   all        105        391      0.858      0.781      0.834      0.524\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      48/49      1.17G    0.03369    0.03153   0.005864        124        416: 100% 50/50 [00:07<00:00,  6.85it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.06it/s]\n",
            "                   all        105        391      0.847      0.798      0.834      0.522\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      49/49      1.17G    0.03339    0.03138   0.005449        148        416: 100% 50/50 [00:08<00:00,  5.69it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.90it/s]\n",
            "                   all        105        391      0.841      0.796      0.837      0.524\n",
            "\n",
            "50 epochs completed in 0.131 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 14.3MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 14.3MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7029004 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:02<00:00,  1.35it/s]\n",
            "                   all        105        391      0.843      0.788      0.835      0.528\n",
            "             ambulance        105         52      0.915      0.923      0.952      0.754\n",
            "                   car        105        175       0.84       0.51      0.706      0.325\n",
            "             firetruck        105         29      0.773      0.942      0.927      0.734\n",
            "             motorbike        105         26          1      0.758      0.874      0.366\n",
            "                person        105         24      0.653      0.708      0.658      0.345\n",
            "            police_car        105         40      0.823       0.85      0.877      0.631\n",
            "     transport_vehicle        105         45      0.895      0.822       0.85      0.539\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python train.py --img 400 --batch 16 --epochs 50 --data {dataset.location}/data.yaml --weights yolov5s.pt --cache --freeze 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TzsNeFaAYnW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWNmhkR8jN9t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcIRLQOlA14A"
      },
      "source": [
        "# Visualize the results with tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kill 35196"
      ],
      "metadata": {
        "id": "7Io-9XrBaktl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jS9_BxdBBHL"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard\n",
        "# Launch after you have started training\n",
        "# logs save in the folder \"runs\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtmS7_TXFsT3"
      },
      "source": [
        "#Run Inference  With Trained Weights\n",
        "Run inference with a pretrained checkpoint on contents of `test/images` folder downloaded from Roboflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWjjiBcic3Vz",
        "outputId": "f6a7e914-87f6-4fdd-c295-704f6d595137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp/weights/best.pt'], source=/content/datasets/smart-traffic-light-26/test/images, data=data/coco128.yaml, imgsz=[416, 416], conf_thres=0.1, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-117-g85f6019 Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7029004 parameters, 0 gradients, 15.8 GFLOPs\n",
            "image 1/127 /content/datasets/smart-traffic-light-26/test/images/1101228810-unfall-2Fu7r457FUea_jpg.rf.6265928903734c5cf3a99a1fe26fe669.jpg: 416x416 1 ambulance, 1 firetruck, 3 persons, 9.9ms\n",
            "image 2/127 /content/datasets/smart-traffic-light-26/test/images/11601938-blau_jpg.rf.74047207534cd4071f9bacbd8aa1fb56.jpg: 416x416 3 police_cars, 9.5ms\n",
            "image 3/127 /content/datasets/smart-traffic-light-26/test/images/1175_jpg.rf.3aa62a92d25679e2bc379566f8ca2f78.jpg: 416x416 1 firetruck, 8.5ms\n",
            "image 4/127 /content/datasets/smart-traffic-light-26/test/images/13_jpeg.rf.5a216563ad174f9b8cc23eaedc5d35c3.jpg: 416x416 1 ambulance, 4 persons, 8.8ms\n",
            "image 5/127 /content/datasets/smart-traffic-light-26/test/images/16_jpeg.rf.cd34c5f596a51114f2eb69fbe2ada6cc.jpg: 416x416 3 transport_vehicles, 9.1ms\n",
            "image 6/127 /content/datasets/smart-traffic-light-26/test/images/171315-large_jpg.rf.4c805a086f941a15f8c45de86845ff56.jpg: 416x416 2 firetrucks, 8.7ms\n",
            "image 7/127 /content/datasets/smart-traffic-light-26/test/images/17_jpeg.rf.7ecd88dd5e8f8f7da0873e958b89a22b.jpg: 416x416 1 police_car, 9.0ms\n",
            "image 8/127 /content/datasets/smart-traffic-light-26/test/images/1_jpeg.rf.26fdb0119df50e8089cabc2ee4ab2832.jpg: 416x416 2 ambulances, 7 cars, 1 firetruck, 2 transport_vehicles, 9.0ms\n",
            "image 9/127 /content/datasets/smart-traffic-light-26/test/images/2021-07-02_-_FustW_-_SH32415_jpg.rf.5b6249574625fc2ba12e57c249034f9e.jpg: 416x416 1 ambulance, 1 car, 1 police_car, 8.9ms\n",
            "image 10/127 /content/datasets/smart-traffic-light-26/test/images/212ce94d-321a-4624-a9ab-3c68fbd6fa4c_jpeg.rf.2466f76fd4a347f11cd2259d2217756e.jpg: 416x416 1 ambulance, 9.0ms\n",
            "image 11/127 /content/datasets/smart-traffic-light-26/test/images/29_jpeg.rf.3dff8c3a4e664a06b2a497977001e764.jpg: 416x416 2 cars, 1 firetruck, 2 police_cars, 9.6ms\n",
            "image 12/127 /content/datasets/smart-traffic-light-26/test/images/30_jpeg.rf.ecae423e57f2a86bc01f20ea6937dcd0.jpg: 416x416 1 ambulance, 2 persons, 1 police_car, 8.7ms\n",
            "image 13/127 /content/datasets/smart-traffic-light-26/test/images/34_jpeg.rf.1822777c8446debcd91543d62ad487b3.jpg: 416x416 2 ambulances, 8.8ms\n",
            "image 14/127 /content/datasets/smart-traffic-light-26/test/images/53767639_605_jpg.rf.c5f2f4c3bdbe899cd6cd2fd73d24133d.jpg: 416x416 2 persons, 2 police_cars, 9.2ms\n",
            "image 15/127 /content/datasets/smart-traffic-light-26/test/images/57305323_605_jpg.rf.0f2866bd6c43c18ecf2b98bf6895c800.jpg: 416x416 3 police_cars, 13.4ms\n",
            "image 16/127 /content/datasets/smart-traffic-light-26/test/images/58299858_1006_jpeg.rf.e7f452e1eafaa9446bf9c3a4f198ccc1.jpg: 416x416 1 firetruck, 1 police_car, 1 transport_vehicle, 8.8ms\n",
            "image 17/127 /content/datasets/smart-traffic-light-26/test/images/5_jpeg.rf.b69a12385cea04959ed765665c3a1097.jpg: 416x416 1 ambulance, 7 cars, 2 transport_vehicles, 9.1ms\n",
            "image 18/127 /content/datasets/smart-traffic-light-26/test/images/66631873_2444354912292111_3871429279472418816_o-1-_jpg.rf.2055c4126861d933e9a2c7fc351e4c95.jpg: 416x416 1 ambulance, 1 firetruck, 1 person, 9.1ms\n",
            "image 19/127 /content/datasets/smart-traffic-light-26/test/images/6_jpeg.rf.0687bcf4628bd9c8c1b2059988bf5f91.jpg: 416x416 1 ambulance, 2 cars, 1 police_car, 9.1ms\n",
            "image 20/127 /content/datasets/smart-traffic-light-26/test/images/6_jpeg.rf.ea48db4ecf8312d55a95a7ba07c8ca97.jpg: 416x416 1 police_car, 8.8ms\n",
            "image 21/127 /content/datasets/smart-traffic-light-26/test/images/71123312-German-Autobahn-traffic-jam-congestion-A-99-cars-trucks-stopped-halt-motorway-highway-freeway-speed-speed_jpg.rf.54642ae63ba5d05da0c157a3fd3d0d23.jpg: 416x416 6 cars, 3 motorbikes, 4 transport_vehicles, 9.6ms\n",
            "image 22/127 /content/datasets/smart-traffic-light-26/test/images/CBGsSX6W4AE4Y0G_jpeg.rf.fb6088d68b89cfaf0d8e8935a949cd7e.jpg: 416x416 11 ambulances, 2 transport_vehicles, 9.3ms\n",
            "image 23/127 /content/datasets/smart-traffic-light-26/test/images/DSCN7933-676x479_jpg.rf.55923272fdb7bd6bd9458e11c3cb0c69.jpg: 416x416 1 police_car, 8.9ms\n",
            "image 24/127 /content/datasets/smart-traffic-light-26/test/images/Firetruck20_jpeg.rf.3b86f837d0aba024e441401827d1e8db.jpg: 416x416 1 ambulance, 3 firetrucks, 2 persons, 2 police_cars, 1 transport_vehicle, 11.8ms\n",
            "image 25/127 /content/datasets/smart-traffic-light-26/test/images/Image_10_jpg.rf.89fa44266215884208a26da140d9f32c.jpg: 416x416 1 firetruck, 1 transport_vehicle, 13.6ms\n",
            "image 26/127 /content/datasets/smart-traffic-light-26/test/images/Image_13_jpg.rf.09dbb410dd7fa1a5b129277ace05748d.jpg: 416x416 1 firetruck, 9.2ms\n",
            "image 27/127 /content/datasets/smart-traffic-light-26/test/images/Image_141_jpg.rf.3389eaef57f5f19af004be41bdcb0e29.jpg: 416x416 1 firetruck, 9.9ms\n",
            "image 28/127 /content/datasets/smart-traffic-light-26/test/images/Image_155_jpg.rf.287c1ee370cc4d25e0df1b2c86f1a2d4.jpg: 416x416 1 ambulance, 1 firetruck, 9.3ms\n",
            "image 29/127 /content/datasets/smart-traffic-light-26/test/images/Image_15_jpg.rf.778af6c75a323e28c3caac14acdc4bb0.jpg: 416x416 2 firetrucks, 9.1ms\n",
            "image 30/127 /content/datasets/smart-traffic-light-26/test/images/Image_17_jpg.rf.068243777be957ebb01c738ec6c79803.jpg: 416x416 2 firetrucks, 9.3ms\n",
            "image 31/127 /content/datasets/smart-traffic-light-26/test/images/Image_18_jpg.rf.e345a619c9c0f71511922e33f1e66721.jpg: 416x416 1 firetruck, 9.0ms\n",
            "image 32/127 /content/datasets/smart-traffic-light-26/test/images/Image_194_jpg.rf.01281c279c80d1afa7cca37a14ec4c5b.jpg: 416x416 1 firetruck, 10.2ms\n",
            "image 33/127 /content/datasets/smart-traffic-light-26/test/images/Image_1_jpg.rf.bcf19d358e891a187004fdee20efe2aa.jpg: 416x416 1 firetruck, 8.8ms\n",
            "image 34/127 /content/datasets/smart-traffic-light-26/test/images/Image_205_jpg.rf.3a6c8ad793ef040ebcacccbd9d4cb243.jpg: 416x416 1 firetruck, 8.8ms\n",
            "image 35/127 /content/datasets/smart-traffic-light-26/test/images/Image_21_jpg.rf.75ef319e609324df3c3c7b2240b5a73a.jpg: 416x416 1 firetruck, 9.4ms\n",
            "image 36/127 /content/datasets/smart-traffic-light-26/test/images/Image_235_jpg.rf.2b4b947778e28f618c9dfcb42da9dc97.jpg: 416x416 1 car, 1 firetruck, 1 person, 10.4ms\n",
            "image 37/127 /content/datasets/smart-traffic-light-26/test/images/Image_237_png.rf.11fcf8d5b065cb1964b368f2e2f370e6.jpg: 416x416 1 ambulance, 1 car, 1 firetruck, 1 police_car, 9.2ms\n",
            "image 38/127 /content/datasets/smart-traffic-light-26/test/images/Image_238_jpg.rf.ec911177d22a1e504575836eb86f6a7e.jpg: 416x416 1 firetruck, 9.2ms\n",
            "image 39/127 /content/datasets/smart-traffic-light-26/test/images/Image_23_jpg.rf.943545611d15b492099944e0fe093e78.jpg: 416x416 1 firetruck, 9.1ms\n",
            "image 40/127 /content/datasets/smart-traffic-light-26/test/images/Image_26_jpg.rf.409c93a8f7afb0849f420fb4bc7a5fee.jpg: 416x416 1 firetruck, 9.4ms\n",
            "image 41/127 /content/datasets/smart-traffic-light-26/test/images/Image_28_jpg.rf.2f1a020c264626cc9a9485753263deb3.jpg: 416x416 1 firetruck, 2 persons, 9.0ms\n",
            "image 42/127 /content/datasets/smart-traffic-light-26/test/images/Image_319_jpg.rf.0cf3503bbdca326e69e82483e8547490.jpg: 416x416 1 firetruck, 1 person, 8.7ms\n",
            "image 43/127 /content/datasets/smart-traffic-light-26/test/images/Image_32_jpg.rf.b48540ace5723b26dd3056716ce84d28.jpg: 416x416 1 firetruck, 9.0ms\n",
            "image 44/127 /content/datasets/smart-traffic-light-26/test/images/Image_3_jpg.rf.fa182a8e3d8e2057f2b4a81c01edc2af.jpg: 416x416 1 firetruck, 11.7ms\n",
            "image 45/127 /content/datasets/smart-traffic-light-26/test/images/Image_40_jpg.rf.59e3d7b7a580ed9c2949bfe85ecbf643.jpg: 416x416 2 ambulances, 2 firetrucks, 5 persons, 9.0ms\n",
            "image 46/127 /content/datasets/smart-traffic-light-26/test/images/Image_40_jpg.rf.d2bf836ac1e35178db5e66fc0189d762.jpg: 416x416 1 firetruck, 9.1ms\n",
            "image 47/127 /content/datasets/smart-traffic-light-26/test/images/Image_48_jpg.rf.10afbb9a6ec360f81830079238bbda14.jpg: 416x416 1 firetruck, 1 person, 1 police_car, 9.3ms\n",
            "image 48/127 /content/datasets/smart-traffic-light-26/test/images/Image_52_jpg.rf.11bc96fb2a56f6e88775cf80c9cb390b.jpg: 416x416 2 firetrucks, 9.0ms\n",
            "image 49/127 /content/datasets/smart-traffic-light-26/test/images/Image_6_jpg.rf.1dda47257e17acbce5bb95abd12d94aa.jpg: 416x416 2 firetrucks, 9.2ms\n",
            "image 50/127 /content/datasets/smart-traffic-light-26/test/images/Image_8_jpg.rf.f8841dd1a12c43d5e31581f2611d40fc.jpg: 416x416 1 firetruck, 5 persons, 8.8ms\n",
            "image 51/127 /content/datasets/smart-traffic-light-26/test/images/Image_93_jpg.rf.493e61f9b326fae9dd34106c1f486f7a.jpg: 416x416 1 firetruck, 9.0ms\n",
            "image 52/127 /content/datasets/smart-traffic-light-26/test/images/Image_9_jpg.rf.ce9b5615c209427f4dfa6e16fc840acd.jpg: 416x416 1 firetruck, 8.9ms\n",
            "image 53/127 /content/datasets/smart-traffic-light-26/test/images/Karlsruhe_SkateNite_1_jpg.rf.80d0ed8a9beab09fe3af3e774c90da07.jpg: 416x416 4 motorbikes, 1 person, 1 police_car, 8.9ms\n",
            "image 54/127 /content/datasets/smart-traffic-light-26/test/images/Moto3-magazine-2018-auteur-Carl-Hocquart_1656x1102-1920x1080-c_jpg.rf.e5c431a85424e15d2e3a3b6014ad0428.jpg: 416x416 2 motorbikes, 2 persons, 12.1ms\n",
            "image 55/127 /content/datasets/smart-traffic-light-26/test/images/NEF1BFIserlohn_png.rf.6a5a1cb3d6663fab4f2afa475299f7d7.jpg: 416x416 1 ambulance, 8.9ms\n",
            "image 56/127 /content/datasets/smart-traffic-light-26/test/images/P90291676-three-generations-of-mini-police-vehicles-used-by-the-munich-police-02-2018-2558px_jpg.rf.86815da9a30b2ad32828863eb026f940.jpg: 416x416 3 police_cars, 8.7ms\n",
            "image 57/127 /content/datasets/smart-traffic-light-26/test/images/P90436911-international-german-motorcycle-championship-idm-2021-bmw-m4-gts-safety-and-medical-car-bmw-m-1000-r-600px_jpg.rf.698478f3f26c0e9ac2e226aa684027b5.jpg: 416x416 5 motorbikes, 1 police_car, 9.5ms\n",
            "image 58/127 /content/datasets/smart-traffic-light-26/test/images/People2_jpeg.rf.f5d54f01e57b0b4cd98258155a4e0730.jpg: 416x416 1 ambulance, 2 firetrucks, 23 persons, 9.7ms\n",
            "image 59/127 /content/datasets/smart-traffic-light-26/test/images/PolNRW2009-074_jpg.rf.4f116c14afcc39cff5e816d542a8f0c6.jpg: 416x416 14 cars, 2 persons, 1 police_car, 2 transport_vehicles, 9.2ms\n",
            "image 60/127 /content/datasets/smart-traffic-light-26/test/images/Polizei-Niedersachsen_jpg.rf.b30fa5cb20ef6744fd931e170d5b8109.jpg: 416x416 2 police_cars, 12.1ms\n",
            "image 61/127 /content/datasets/smart-traffic-light-26/test/images/Polizei_Hessen_Karriere_Autobahnpolizei_23_jpg.rf.3ba38c95837c70d0dc11047b7b8bda57.jpg: 416x416 1 police_car, 9.2ms\n",
            "image 62/127 /content/datasets/smart-traffic-light-26/test/images/RTW-Rettungswagen-Ambulanzauto-4_jpg.rf.2c638643ccd419737f63a222eda772d7.jpg: 416x416 1 ambulance, 9.2ms\n",
            "image 63/127 /content/datasets/smart-traffic-light-26/test/images/RW14_Colorkey_jpeg.rf.b23151d513af3f628b85b8426c06c408.jpg: 416x416 5 ambulances, 2 transport_vehicles, 8.9ms\n",
            "image 64/127 /content/datasets/smart-traffic-light-26/test/images/RW_Busdorf_20170323-620x264_jpg.rf.dfdd13ea21e485a279116bf1a0cd3575.jpg: 416x416 5 ambulances, 3 persons, 13.6ms\n",
            "image 65/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-1-25-11-PM_png.rf.6f02e3ec35caae6702da7fefad397f05.jpg: 416x416 3 firetrucks, 9.0ms\n",
            "image 66/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-1-26-29-PM_png.rf.65eae233adca816100bab4c811ad3b82.jpg: 416x416 1 firetruck, 10.5ms\n",
            "image 67/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-1-27-07-PM_png.rf.64933ed3d20430fe1fba125c8750e538.jpg: 416x416 4 firetrucks, 9.0ms\n",
            "image 68/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-1-28-07-PM_png.rf.a229ae859edf5432411f9b8522e51f5c.jpg: 416x416 1 ambulance, 1 firetruck, 8.7ms\n",
            "image 69/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-2-00-15-PM_png.rf.124f3b6cfc52b86cdaf27d8fc7f2321f.jpg: 416x416 4 police_cars, 1 transport_vehicle, 8.9ms\n",
            "image 70/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-2-02-26-PM_png.rf.2bd59f3f281fe5b2c033ad987bfe622e.jpg: 416x416 4 cars, 1 police_car, 1 transport_vehicle, 10.0ms\n",
            "image 71/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-2-02-55-PM_png.rf.9aced9359d8229594740ba839b8cdec1.jpg: 416x416 1 person, 1 police_car, 11.7ms\n",
            "image 72/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-2-05-12-PM_png.rf.5379091e1bfe678a7fbcbeeb184438fe.jpg: 416x416 3 police_cars, 9.2ms\n",
            "image 73/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-2-07-11-PM_png.rf.df05b8b4b1a94748748ee951f9344022.jpg: 416x416 3 police_cars, 9.3ms\n",
            "image 74/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-3-54-34-PM_png.rf.d0d434a7de69718806c6ff4c0edf3b06.jpg: 416x416 1 police_car, 10.0ms\n",
            "image 75/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-3-56-59-PM_png.rf.bdbd73aaced25b9c551e7e3cc86ecee6.jpg: 416x416 1 ambulance, 10.6ms\n",
            "image 76/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-3-57-26-PM_png.rf.318d7af5818a794d73004dfaf0aaa67f.jpg: 416x416 2 ambulances, 8.7ms\n",
            "image 77/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-3-59-38-PM_png.rf.4ea37f2cc4c45e20ea3d70e31f43d349.jpg: 416x416 1 ambulance, 1 car, 1 police_car, 10.4ms\n",
            "image 78/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-4-00-46-PM_png.rf.85471a0e9464699a9b2c672327d2f84b.jpg: 416x416 1 ambulance, 1 car, 8.9ms\n",
            "image 79/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-4-01-43-PM_png.rf.1ad2c259054a00b8e0b44692a1186bad.jpg: 416x416 1 ambulance, 9.1ms\n",
            "image 80/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-4-02-07-PM_png.rf.a001fcf1b5baff5c5053847d730ccb94.jpg: 416x416 1 ambulance, 1 car, 9.0ms\n",
            "image 81/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-4-02-40-PM_png.rf.4fe2cf7b3526f193f27893af33af2600.jpg: 416x416 2 ambulances, 1 person, 9.3ms\n",
            "image 82/127 /content/datasets/smart-traffic-light-26/test/images/Screenshot-2023-02-23-at-4-02-49-PM_png.rf.8cba63bd9a31fa6c24e4a7b40ba8d63d.jpg: 416x416 1 ambulance, 2 police_cars, 8.5ms\n",
            "image 83/127 /content/datasets/smart-traffic-light-26/test/images/Top-10-Fastest-Police-Cars-in-the-World-5_jpg.rf.0a9f7179e284d08eb573e6cbf8967e45.jpg: 416x416 1 police_car, 10.6ms\n",
            "image 84/127 /content/datasets/smart-traffic-light-26/test/images/_114016685_berlin2_jpg.rf.7b1810b8a1c786afe6583b10164b8477.jpg: 416x416 3 persons, 2 police_cars, 8.7ms\n",
            "image 85/127 /content/datasets/smart-traffic-light-26/test/images/autobahn-neumnster_jpg.rf.a829f22d2bc9ff0aa23c00395449113f.jpg: 416x416 3 persons, 1 police_car, 9.2ms\n",
            "image 86/127 /content/datasets/smart-traffic-light-26/test/images/autobahn_polizei_vpi_bayreuth_jpg.rf.d4516d1fa8dccff9053a5fc6db1cc647.jpg: 416x416 1 police_car, 1 transport_vehicle, 9.1ms\n",
            "image 87/127 /content/datasets/smart-traffic-light-26/test/images/bayern_rettungsdienst_01_jpg.rf.d22f403ecbffa7ade314b1d293e440e4.jpg: 416x416 1 ambulance, 1 firetruck, 3 persons, 9.5ms\n",
            "image 88/127 /content/datasets/smart-traffic-light-26/test/images/bike5_jpeg.rf.5f148b86b4e91cf806c1133638756989.jpg: 416x416 3 motorbikes, 1 police_car, 8.6ms\n",
            "image 89/127 /content/datasets/smart-traffic-light-26/test/images/bus10_jpeg.rf.89727138bfed85d408a1e2004bea7067.jpg: 416x416 1 firetruck, 8.8ms\n",
            "image 90/127 /content/datasets/smart-traffic-light-26/test/images/csm_AOE_inkl-_Fahrzeuge_7271d23276_jpg.rf.c3bd6d92ae2a39637e759e57d76ab3dd.jpg: 416x416 5 ambulances, 9.0ms\n",
            "image 91/127 /content/datasets/smart-traffic-light-26/test/images/csm_rb_NL_BFFrankfirtIMG_5877_4d6cb040e8_jpeg.rf.f4e4f8a7f2475664820d0cefd283d709.jpg: 416x416 2 firetrucks, 8.8ms\n",
            "image 92/127 /content/datasets/smart-traffic-light-26/test/images/download_jpeg.rf.c6d9107e2c69ab48ad80f216b917e912.jpg: 416x416 1 police_car, 1 transport_vehicle, 10.4ms\n",
            "image 93/127 /content/datasets/smart-traffic-light-26/test/images/dpa-the-picture-shows-the-wreck-of-a-motor-coach-on-the-a3-highway-D3KT8J_jpg.rf.bb8279078f065613b57fd64b863d141f.jpg: 416x416 3 cars, 1 firetruck, 8 persons, 3 transport_vehicles, 9.1ms\n",
            "image 94/127 /content/datasets/smart-traffic-light-26/test/images/ezgif-frame-011_jpg.rf.f0ce9c21cb6bd9e27a0fbaca1aa82d92.jpg: 416x416 23 cars, 4 motorbikes, 2 transport_vehicles, 10.6ms\n",
            "image 95/127 /content/datasets/smart-traffic-light-26/test/images/ezgif-frame-020_jpg.rf.e5a7a072157a56ce8d22fba9ba748552.jpg: 416x416 23 cars, 4 motorbikes, 3 transport_vehicles, 10.0ms\n",
            "image 96/127 /content/datasets/smart-traffic-light-26/test/images/ezgif-frame-022_jpg.rf.ff06368cb5786ce8574fdc5e3ad023b3.jpg: 416x416 22 cars, 5 motorbikes, 2 transport_vehicles, 14.3ms\n",
            "image 97/127 /content/datasets/smart-traffic-light-26/test/images/gaffer-behindert-rettungskraefte_jpg.rf.371d0907ff7540460d41ce04db8bc440.jpg: 416x416 6 cars, 4 persons, 1 police_car, 9.2ms\n",
            "image 98/127 /content/datasets/smart-traffic-light-26/test/images/gettyimages-1219688923-612x612_jpg.rf.ae367208206df6f5855b65196d579fbe.jpg: 416x416 1 ambulance, 9.2ms\n",
            "image 99/127 /content/datasets/smart-traffic-light-26/test/images/gettyimages-1232404193-1024x1024_jpg.rf.d5288ab0f8859b975872fa7987dcc29b.jpg: 416x416 5 persons, 1 police_car, 9.5ms\n",
            "image 100/127 /content/datasets/smart-traffic-light-26/test/images/gettyimages-1233507735-1024x1024_jpg.rf.4af8e1973f0f7952cd1d77f8316d884a.jpg: 416x416 1 person, 1 police_car, 9.1ms\n",
            "image 101/127 /content/datasets/smart-traffic-light-26/test/images/gettyimages-1301026378-1024x1024_jpg.rf.9917e0e15d66e76b3d3d7a7152e5fd44.jpg: 416x416 1 car, 2 police_cars, 9.9ms\n",
            "image 102/127 /content/datasets/smart-traffic-light-26/test/images/gettyimages-1437309131-612x612_jpg.rf.e113c3e9564a69a928a5d1a98f3d0ee5.jpg: 416x416 1 ambulance, 8 cars, 1 firetruck, 8.9ms\n",
            "image 103/127 /content/datasets/smart-traffic-light-26/test/images/gettyimages-458560627-1024x1024_jpg.rf.2ef038f02f4c280a4d011a05a1cc2fd0.jpg: 416x416 4 cars, 2 firetrucks, 1 police_car, 5 transport_vehicles, 12.4ms\n",
            "image 104/127 /content/datasets/smart-traffic-light-26/test/images/gettyimages-844899246-612x612_jpg.rf.365d3c498fc383780c2bea746ba6a540.jpg: 416x416 1 car, 7 motorbikes, 2 persons, 1 police_car, 9.3ms\n",
            "image 105/127 /content/datasets/smart-traffic-light-26/test/images/hqdefault_jpg.rf.5d158e4e5d33c7da5d2b95eead4016c5.jpg: 416x416 1 police_car, 11.4ms\n",
            "image 106/127 /content/datasets/smart-traffic-light-26/test/images/ibr-1891216_jpg.rf.9a78042db62b4916c02bac336d7545b6.jpg: 416x416 3 ambulances, 7 cars, 3 transport_vehicles, 12.7ms\n",
            "image 107/127 /content/datasets/smart-traffic-light-26/test/images/images-1-_jpeg.rf.102c24a62954db62a849d894f2441b7e.jpg: 416x416 2 ambulances, 1 firetruck, 9.5ms\n",
            "image 108/127 /content/datasets/smart-traffic-light-26/test/images/images-1-_jpeg.rf.acca2b0a43ee00139bbd9011a0e3e2ef.jpg: 416x416 1 car, 2 police_cars, 9.2ms\n",
            "image 109/127 /content/datasets/smart-traffic-light-26/test/images/images-1-_jpeg.rf.dc789a4b3333efa6672add53102a91e9.jpg: 416x416 1 firetruck, 2 police_cars, 10.6ms\n",
            "image 110/127 /content/datasets/smart-traffic-light-26/test/images/images-13-_jpeg.rf.6c9ca0720199f7797dade6434b544108.jpg: 416x416 3 ambulances, 9.0ms\n",
            "image 111/127 /content/datasets/smart-traffic-light-26/test/images/images-2-_jpeg.rf.140eaa45a4596dd1443647a1d82b2f70.jpg: 416x416 3 persons, 3 police_cars, 8.6ms\n",
            "image 112/127 /content/datasets/smart-traffic-light-26/test/images/images-2-_jpeg.rf.7b45299384a4e22efd810fd3fc563db4.jpg: 416x416 1 person, 1 police_car, 2 transport_vehicles, 9.1ms\n",
            "image 113/127 /content/datasets/smart-traffic-light-26/test/images/images-6-_jpeg.rf.afd1c96e65cbfea136b269bd151a89ea.jpg: 416x416 1 police_car, 1 transport_vehicle, 9.9ms\n",
            "image 114/127 /content/datasets/smart-traffic-light-26/test/images/images_jpeg.rf.831c682c70c212f3efd6df42d378cd67.jpg: 416x416 1 ambulance, 9.0ms\n",
            "image 115/127 /content/datasets/smart-traffic-light-26/test/images/jonas-augustin-0AGmGu5U08k-unsplash_jpg.rf.43705f4cc846e351ff437fc03cfe479a.jpg: 416x416 1 ambulance, 9.2ms\n",
            "image 116/127 /content/datasets/smart-traffic-light-26/test/images/large_Ol-HJOQ0n_gv2lxk2V763lH_22Ho3rnALabCUNdZydg_jpg.rf.f360168a01e38cd191045f3d2b953beb.jpg: 416x416 1 ambulance, 28 cars, 4 transport_vehicles, 9.6ms\n",
            "image 117/127 /content/datasets/smart-traffic-light-26/test/images/lf-8-12-5_jpg.rf.b9c0f68e781615b3534fc7b8aacc9898.jpg: 416x416 1 firetruck, 9.5ms\n",
            "image 118/127 /content/datasets/smart-traffic-light-26/test/images/maxresdefault-1-_jpg.rf.05a43a62365194f87ac82a65dd727209.jpg: 416x416 1 ambulance, 1 police_car, 9.4ms\n",
            "image 119/127 /content/datasets/smart-traffic-light-26/test/images/maxresdefault-1-_jpg.rf.6214469110e09035cf82c0d658b9346f.jpg: 416x416 2 cars, 1 police_car, 1 transport_vehicle, 9.9ms\n",
            "image 120/127 /content/datasets/smart-traffic-light-26/test/images/maxresdefault_jpg.rf.9c926797338d208f131a2ec7cc0bf968.jpg: 416x416 1 car, 1 police_car, 1 transport_vehicle, 9.3ms\n",
            "image 121/127 /content/datasets/smart-traffic-light-26/test/images/pexels-photo-1600757_jpeg.rf.3b08dd59fe59129086ad979c4acebc9c.jpg: 416x416 2 cars, 1 firetruck, 1 motorbike, 3 transport_vehicles, 9.1ms\n",
            "image 122/127 /content/datasets/smart-traffic-light-26/test/images/police-cars-street-berlin-germany-new-33506074_jpg.rf.55138a5ae6970cda73443560b16bb826.jpg: 416x416 2 police_cars, 9.3ms\n",
            "image 123/127 /content/datasets/smart-traffic-light-26/test/images/police13_jpeg.rf.4bc90db52a2d6cb7ce2cc6084c4d4f36.jpg: 416x416 3 police_cars, 11.7ms\n",
            "image 124/127 /content/datasets/smart-traffic-light-26/test/images/rettungsdienst_flotte_jpg.rf.e84d11be0b618f46e935e146ce7fa630.jpg: 416x416 4 ambulances, 9.2ms\n",
            "image 125/127 /content/datasets/smart-traffic-light-26/test/images/stau-auf-der-autobahn-polizeiauto-fahren-durch-eine-gasse-erstellt-von-den-anderen-autos-a-81-stuttgart-zuffenhausen-b6155y_jpg.rf.27b65802ff4e6ab5aa444e92ffa169b8.jpg: 416x416 2 persons, 3 police_cars, 10.0ms\n",
            "image 126/127 /content/datasets/smart-traffic-light-26/test/images/traffic-rises_jpg.rf.17ffbf1c45913b2cb916c819b5650f8d.jpg: 416x416 3 cars, 2 transport_vehicles, 9.0ms\n",
            "image 127/127 /content/datasets/smart-traffic-light-26/test/images/vw_polizei_niedersachsen_17_700_jpg.rf.bf292d3acd23283863fb63ffe84fbce4.jpg: 416x416 6 persons, 2 police_cars, 8.6ms\n",
            "Speed: 0.4ms pre-process, 9.6ms inference, 1.1ms NMS per image at shape (1, 3, 416, 416)\n",
            "Results saved to \u001b[1mruns/detect/exp2\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python detect.py --weights runs/train/exp/weights/best.pt --img 400 --conf 0.1 --source /content/datasets/smart-traffic-light-26/test/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbUn4_b9GCKO"
      },
      "outputs": [],
      "source": [
        "#display inference on ALL test images\n",
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for imageName in glob.glob('/content/yolov5/runs/detect/exp/*.jpg'): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7iiObB2WCMh6",
        "outputId": "6cc380bc-f204-4a88-c5a4-99e2d011f602"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7472428d-d363-40f2-adea-7a7876bb23fb\", \"best.pt\", 14340477)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#export your model's weights for future use\n",
        "from google.colab import files\n",
        "files.download('./runs/train/exp/weights/best.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNn-obvOGITm"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DYvTnbIu6lK"
      },
      "outputs": [],
      "source": [
        "files.download('/content/yolov5/runs/detect/exp4/pexels-kelly-lacy-5473765.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rJfXhYvYM5Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Faster R-CNN "
      ],
      "metadata": {
        "id": "fEt7iRXmM7yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(model_format=\"yolov5\", notebook=\"ultralytics\")\n",
        "\n",
        "# set up environment\n",
        "os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\"\n",
        "\n",
        "#after following the link above, recieve python code with these fields filled in\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"HDll2oFm5X8KK35ziZJL\")\n",
        "project = rf.workspace().project(\"casestudy1/smart-traffic-light\")\n",
        "dataset = project.version(\"16\").download(\"yolov5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqJeYwTXM5To",
        "outputId": "cf85ac9b-b3a7-48cc-ae22-d13aab07aa5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "upload and label your dataset, and get an API KEY here: https://app.roboflow.com/?model=yolov5&ref=ultralytics\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Exporting format yolov5pytorch in progress : 50.0%\n",
            "Version export complete for yolov5pytorch format\n",
            "Downloading Dataset Version Zip in /content/datasets/smart-traffic-light-16 to yolov5pytorch: 100% [49252546 / 49252546] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to /content/datasets/smart-traffic-light-16 in yolov5pytorch:: 100%|██████████| 1434/1434 [00:00<00:00, 1983.17it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo {dataset.location}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_KxX79aM5Q0",
        "outputId": "c40b5bc3-7597-4043-dc7a-154a1d03ab63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets/smart-traffic-light-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = { 0: \"ambulance\", 1: \"car\", 2: \"firetruck\", 3: \"motorbike\", 4: \"person\", 5: \"police_car\", 6:\"transport_vehicle\"}\n",
        "labels_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ca0KJKcP2Tj",
        "outputId": "5a769b2e-1b2f-40f8-dc1b-1c3e4623072f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'ambulance',\n",
              " 1: 'car',\n",
              " 2: 'firetruck',\n",
              " 3: 'motorbike',\n",
              " 4: 'person',\n",
              " 5: 'police_car',\n",
              " 6: 'transport_vehicle'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def txt_to_dict(txt_path):\n",
        "    with open(txt_path, 'r') as label_file:\n",
        "      file_content = label_file.readlines()\n",
        "    file_content    \n",
        "    return {\"filename\": xml_path,\n",
        "            \"image_width\": 640,\n",
        "            \"image_height\": 640,\n",
        "            \"image_channels\": int(root.find(\"./size/depth\").text),\n",
        "            \"label\": root.find(\"./object/name\").text,\n",
        "            \"x1\": int(root.find(\"./object/bndbox/xmin\").text),\n",
        "            \"y1\": int(root.find(\"./object/bndbox/ymin\").text),\n",
        "            \"x2\": int(root.find(\"./object/bndbox/xmax\").text),\n",
        "            \"y2\": int(root.find(\"./object/bndbox/ymax\").text)}"
      ],
      "metadata": {
        "id": "bq3mxtLUM5OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = \"/content/datasets/smart-traffic-light-16\""
      ],
      "metadata": {
        "id": "uelVuRw5M5L5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/datasets/smart-traffic-light-16/train/labels/10-Drehleitern-Frankfurt_Gruppenfoto-von-oben_jpg.rf.0c2d68e22da36e4b7a889361e2ee8134.txt\"\n",
        "with open(path) as file:\n",
        "  x = file.readlines()"
      ],
      "metadata": {
        "id": "EZskTWiBM5JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O6y_COVM5Gc",
        "outputId": "63d501e8-0fba-4181-ab9a-48d1deb1d0e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2 0.18125 0.5234375 0.1453125 0.16640625\\n',\n",
              " '2 0.2578125 0.47578125 0.115625 0.13359375\\n',\n",
              " '2 0.3328125 0.4515625 0.09375 0.10390625\\n',\n",
              " '2 0.425 0.4390625 0.06015625 0.08984375\\n',\n",
              " '2 0.5 0.44296875 0.0546875 0.09140625\\n',\n",
              " '2 0.5703125 0.440625 0.0578125 0.09140625\\n',\n",
              " '2 0.64609375 0.45703125 0.07109375 0.1\\n',\n",
              " '2 0.740625 0.471875 0.1046875 0.1109375\\n',\n",
              " '2 0.8171875 0.49921875 0.121875 0.14375\\n',\n",
              " '2 0.4890625 0.72890625 0.340625 0.32421875']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBsPBtMOM5Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dAVcz61ZM5AH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}